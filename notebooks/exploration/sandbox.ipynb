{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from shrugger.src.model import load_model\n",
    "\n",
    "# Use absolute path to config.json in the project root\n",
    "config_path = \"../../config.json\"\n",
    "with open(config_path) as f:\n",
    "    cfg = json.load(f)\n",
    "model_id = cfg[\"model_id\"]\n",
    "dtype = cfg[\"dtype\"]\n",
    "\n",
    "tokenizer, model = load_model(model_id, dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.eval_utils import run_likert_probe\n",
    "\n",
    "run_likert_probe(tokenizer, model, \"Is parity conserved in weak nuclear interactions?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.experiment_utils import *\n",
    "\n",
    "get_questions_by_filter(subject=\"Physics\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.experiment import load_experiment_results, run_likert_experiment\n",
    "from shrugger.src.model import load_model\n",
    "\n",
    "# Load model\n",
    "tokenizer, model = load_model(\"google/gemma-3-4b-it\")\n",
    "\n",
    "# Run experiment on Biology questions\n",
    "stats = run_likert_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_file=\"results/test_nums.json\",\n",
    "    subject=\"Physics\",\n",
    "    difficulty=1,\n",
    "    form=\"V0_numbers\"\n",
    ")\n",
    "\n",
    "if stats['completed_questions'] > 0:\n",
    "    print(f\"Completed {stats['completed_questions']} questions\")\n",
    "    print(f\"Average score: {stats['average_score']:.2f}\")\n",
    "    print(f\"Valid predictions: {stats['valid_predictions']}\")\n",
    "else:\n",
    "    print(\"No new questions processed. All questions were already in the results file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Later, load and analyze results\n",
    "results = load_experiment_results(\"../../results/test_likert.json\")\n",
    "print(f\"Score distribution: {results['score_distribution']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with custom labels\n",
    "custom_labels = [\"X\", \"Y\", \"Z\", \"W\", \"V\"]\n",
    "results = run_likert_probe(tokenizer, model, \"Is the Earth round?\", labels=custom_labels)\n",
    "\n",
    "# The prompt will show:\n",
    "# Question: Is the Earth round?\n",
    "# X) Definitely yes\n",
    "# Y) Probably yes\n",
    "# Z) Not sure\n",
    "# W) Probably no\n",
    "# V) Definitely no\n",
    "# Respond with exactly one of: X, Y, Z, W, V.\n",
    "# Answer:\n",
    "\n",
    "# If the model outputs \"X\", it will be converted to \"YY\" in the results\n",
    "print(results['pred_label'])  # Will be \"YY\" (universal format)\n",
    "print(results['canonical_label'])  # Will be \"X\" (original model output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.probes import analyze_residual_stream, get_all_residual_streams\n",
    "\n",
    "# Get all residual streams\n",
    "all_streams = get_all_residual_streams(model, tokenizer, \"Is the meaning of life the answer to the ultimate question of life, the universe, and everything?\")\n",
    "print(f\"Shape: {all_streams.shape}\")  # [num_layers, seq_len, hidden_dim]\n",
    "\n",
    "# Extract the last token's representation across all layers\n",
    "last_token = all_streams[:, -1, :]\n",
    "\n",
    "# Analyze using PCA for visualization\n",
    "reduced = analyze_residual_stream(all_streams, reduction=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.residual_viz import *\n",
    "\n",
    "# 1) Norm heatmap\n",
    "norm_LT = analyze_residual_stream(all_streams, reduction=\"norm\")\n",
    "plot_heatmap_LT(norm_LT, title=\"Residual L2 norm per (layer, token)\", transform=\"log\")\n",
    "\n",
    "# 2) Token path in PCA plane\n",
    "pca_L_T_3 = analyze_residual_stream(all_streams, reduction=\"pca\")\n",
    "plot_token_path_from_pca(pca_L_T_3, token_indices=[-1, -2])\n",
    "\n",
    "# 3) Layer scatter in PCA plane\n",
    "plot_layer_scatter_from_pca(pca_L_T_3, layer_index=10, token_indices=range(0, pca_L_T_3.shape[1], 2))\n",
    "\n",
    "# 4) Per-token series across layers\n",
    "plot_token_series_from_LT(norm_LT, token_indices=[-1, -2], ylabel=\"L2 norm\")\n",
    "\n",
    "# 5) Per-layer series across tokens\n",
    "plot_layer_series_from_LT(norm_LT, layer_indices=[10, 20, 30], ylabel=\"L2 norm\")\n",
    "\n",
    "# 6) Per-layer contributions for a token (bar chart)\n",
    "plot_token_contributions_from_LT(norm_LT, token_indices=[-1, -2], ylabel=\"L2 norm\")\n",
    "\n",
    "# # 7) Δ heatmap (compare two prompts)\n",
    "# norm_A = analyze_residual_stream(residual_stream_A, reduction=\"norm\")\n",
    "# norm_B = analyze_residual_stream(residual_stream_B, reduction=\"norm\")\n",
    "# plot_delta_heatmap_LT(norm_A, norm_B, title=\"Δ L2 norm (B − A)\")\n",
    "\n",
    "# 8) If you started with reduction=\"none\"\n",
    "# rs_none = analyze_residual_stream(all_streams, reduction=\"none\")\n",
    "# norm_from_none = summarize_to_norm(rs_none)\n",
    "# plot_heatmap_LT(norm_from_none, title=\"Norm (from raw)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.residual_viz import plot_token_path_from_pca\n",
    "\n",
    "# Use the same residual streams from previous example\n",
    "# Analyze using PCA for visualization (3 components)\n",
    "pca_L_T_3 = analyze_residual_stream(all_streams, reduction=\"pca\")\n",
    "\n",
    "# Plot the path of the last token through layers in PCA space\n",
    "fig = plot_token_path_from_pca(\n",
    "    pca_L_T_3,\n",
    "    token_indices=[-1, -2],\n",
    "    annotate_layers=True,\n",
    "    title=\"Token Path Through Layers in PCA Space\",\n",
    "    show_arrows=True,\n",
    "    arrow_spacing=3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.residual_viz import plot_layer_scatter_from_pca\n",
    "\n",
    "# Use the same PCA-reduced data from previous example\n",
    "# Plot token representations at a specific layer\n",
    "fig = plot_layer_scatter_from_pca(\n",
    "    pca_L_T_3,\n",
    "    layer_index=33,  # Middle layer\n",
    "    token_indices=slice(None),  # All tokens\n",
    "    title=\"Token Representations at Layer 10\",\n",
    "    color_by=\"index\",  # Color by token index\n",
    "    label_tokens=True,\n",
    "    add_convex_hull=True,\n",
    "    add_centroid=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.residual_viz import plot_token_series_from_LT\n",
    "\n",
    "# Use the norm_LT data from the first example\n",
    "# Plot how a token's norm changes across layers\n",
    "fig = plot_token_series_from_LT(\n",
    "    norm_LT,\n",
    "    token_indices=[-1, -2],  # Last two tokens\n",
    "    title=\"Token Norm Across Layers\",\n",
    "    ylabel=\"L2 Norm\",\n",
    "    show_legend=True,\n",
    "    show_stats=True,\n",
    "    show_min_max=True,\n",
    "    show_average=True,\n",
    "    show_trend=True,\n",
    "    log_scale=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.residual_viz import plot_layer_series_from_LT\n",
    "\n",
    "# Use the norm_LT data from the first example\n",
    "# Plot how a layer's norm changes across tokens\n",
    "fig = plot_layer_series_from_LT(\n",
    "    norm_LT,\n",
    "    layer_indices=[5, 15, 25],  # Early, middle, and late layers\n",
    "    title=\"Layer Norm Across Tokens\",\n",
    "    ylabel=\"L2 Norm\",\n",
    "    show_legend=True,\n",
    "    show_stats=True,\n",
    "    log_scale=True,\n",
    "    highlight_tokens=[-1]  # Highlight the last token\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.residual_viz import plot_token_contributions_from_LT\n",
    "\n",
    "# Use the norm_LT data from the first example\n",
    "# Plot layer-wise contributions for a specific token\n",
    "fig = plot_token_contributions_from_LT(\n",
    "    norm_LT,\n",
    "    token_indices=-1,  # Last token\n",
    "    title=\"Layer Contributions for Last Token\",\n",
    "    ylabel=\"L2 Norm\",\n",
    "    show_values=True,\n",
    "    sort_by_value=True,  # Sort layers by contribution\n",
    "    normalize=True,  # Show as percentage of total\n",
    "    show_stats=True,\n",
    "    highlight_layers=[10, 20, 30]  # Highlight specific layers\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.probes import analyze_residual_stream, get_all_residual_streams\n",
    "from shrugger.src.residual_viz import plot_delta_heatmap_LT\n",
    "\n",
    "# Get residual streams for two different prompts\n",
    "prompt_a = \"What is machine learning?\"\n",
    "prompt_b = \"What are machine learning?\"\n",
    "\n",
    "# Get residual streams for both prompts\n",
    "streams_a = get_all_residual_streams(model, tokenizer, prompt_a)\n",
    "streams_b = get_all_residual_streams(model, tokenizer, prompt_b)\n",
    "\n",
    "# Create layer-token summaries using L2 norm\n",
    "norm_a = analyze_residual_stream(streams_a, reduction=\"norm\")\n",
    "norm_b = analyze_residual_stream(streams_b, reduction=\"norm\")\n",
    "\n",
    "# Plot the difference between the two prompts\n",
    "fig = plot_delta_heatmap_LT(\n",
    "    norm_a,\n",
    "    norm_b,\n",
    "    title=\"Δ L2 Norm (B − A)\",\n",
    "    condition_a_name=\"ML Question\",\n",
    "    condition_b_name=\"DL Question\",\n",
    "    show_colorbar=True,\n",
    "    center_colormap=True,\n",
    "    show_side_plots=True,\n",
    "    show_stats=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrugger.src.experiment import run_combined_experiment\n",
    "from shrugger.src.model import load_model\n",
    "\n",
    "# Load your model\n",
    "tokenizer, model = load_model(\"google/gemma-3-4b-it\")\n",
    "\n",
    "# Run the combined experiment\n",
    "results = run_combined_experiment(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    output_dir=\"./results/my_experiment\",\n",
    "    subject=\"Physics\",  # Optional filter\n",
    "    difficulty=5,\n",
    "    split=\"dev\",\n",
    "    form=\"V2_letters\",  # Use one of the Likert description styles\n",
    "    labels=[\"A\", \"B\", \"C\", \"D\", \"E\"],  # Optional custom labels\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# The results include statistics and file paths\n",
    "print(f\"Processed {results['likert_stats']['completed_questions']} questions\")\n",
    "print(f\"Hidden state shape: {results['hidden_stats']['hidden_state_shape']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shrugger",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
