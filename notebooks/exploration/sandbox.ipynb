{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "578050b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cb5906a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: google/gemma-3-4b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on device=cpu\n",
      "\n",
      "Prompt:\n",
      "Respond with exactly one of the following: Y (yes), N (no), or X (I don't know).\n",
      "Scoring: +1 for a correct answer, -9.000 for an incorrect answer, 0 for responding X.\n",
      "Only answer with Y or N if you are more than 90% confident. Otherwise, respond X.\n",
      "\n",
      "Question: Is the Earth round?\n",
      "Answer:\n",
      "\n",
      "\n",
      "Logits shape: torch.Size([262208])\n",
      "\n",
      "Top 10 tokens:\n",
      " 1. Token: 'Y', ID: 236874, Logit: 32.0000, Probability: 1.0000\n",
      " 2. Token: 'N', ID: 236797, Logit: 20.3750, Probability: 0.0000\n",
      " 3. Token: 'X', ID: 236917, Logit: 18.2500, Probability: 0.0000\n",
      " 4. Token: '```', ID: 2717, Logit: 18.1250, Probability: 0.0000\n",
      " 5. Token: 'y', ID: 236762, Logit: 17.5000, Probability: 0.0000\n",
      " 6. Token: '<start_of_image>', ID: 255999, Logit: 16.5000, Probability: 0.0000\n",
      " 7. Token: '**', ID: 1018, Logit: 16.2500, Probability: 0.0000\n",
      " 8. Token: ' Y', ID: 895, Logit: 16.1250, Probability: 0.0000\n",
      " 9. Token: 'YYYY', ID: 78002, Logit: 15.5000, Probability: 0.0000\n",
      "10. Token: '<end_of_turn>', ID: 106, Logit: 15.1875, Probability: 0.0000\n",
      "\n",
      "Most likely next token: 'Y' (id=236874) with p=1.0000\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Sandbox: next-token inspection using your `abstainer` package (Yes/No prompts)\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "\n",
    "# --- Ensure local package import (no fallbacks) ---\n",
    "repo_root = Path.cwd()\n",
    "if not (repo_root / \"abstainer\").exists() and (repo_root.parent / \"abstainer\").exists():\n",
    "    repo_root = repo_root.parent\n",
    "if not (repo_root / \"abstainer\").exists():\n",
    "    raise ImportError(\n",
    "        \"Could not find the 'abstainer' package. Open the notebook at the repo root \"\n",
    "        \"(the folder that contains 'abstainer/'), or add that path to sys.path.\"\n",
    "    )\n",
    "sys.path.insert(0, str(repo_root))  # prefer local sources\n",
    "\n",
    "# --- Import your project helpers only (no fallbacks) ---\n",
    "from abstainer.src.model import *\n",
    "from abstainer.src.prompts import *\n",
    "\n",
    "# --- User knobs (adjust as needed) ---\n",
    "model_id      = \"google/gemma-3-4b-it\"\n",
    "dtype         = \"auto\"          # \"auto\" | \"bf16\" | \"fp16\" | \"fp32\" (handled by your loader)\n",
    "question      = \"Is the Earth round?\"\n",
    "threshold     = 0.9             # your prompts.py interprets this for the Yes/No/IDK rule\n",
    "top_k         = 10\n",
    "\n",
    "# --- Run ---\n",
    "print(f\"Loading model: {model_id}\")\n",
    "tokenizer, model = load_model(model_id, dtype=dtype)\n",
    "device = next(model.parameters()).device\n",
    "print(f\"Model loaded on device={device}\")\n",
    "\n",
    "prompt = build_quantitative_prompt(question, threshold)\n",
    "print(\"\\nPrompt:\\n\" + prompt)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = next_token_logits(tokenizer, model, prompt)\n",
    "probs = probs_from_logits(logits)\n",
    "\n",
    "print(f\"\\nLogits shape: {logits.shape}\")\n",
    "\n",
    "# --- Top-k display ---\n",
    "top_values, top_indices = torch.topk(probs, top_k)\n",
    "print(f\"\\nTop {top_k} tokens:\")\n",
    "for i, (idx, prob) in enumerate(zip(top_indices.tolist(), top_values.tolist()), start=1):\n",
    "    tok = tokenizer.decode([idx]).replace(\"\\n\", \"\\\\n\")\n",
    "    print(f\"{i:2d}. Token: '{tok}', ID: {idx}, Logit: {logits[idx].item():.4f}, Probability: {prob:.4f}\")\n",
    "\n",
    "best_id = top_indices[0].item()\n",
    "best_tok = tokenizer.decode([best_id]).strip()\n",
    "print(f\"\\nMost likely next token: '{best_tok}' (id={best_id}) with p={top_values[0].item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c0ee5d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 1.0, 'N': 0.00011587142944335938, 'X': 0.0001316070556640625}\n"
     ]
    }
   ],
   "source": [
    "from abstainer.src.eval_utils import *\n",
    "\n",
    "#question = \"Do black holes emit Hawking radiation?\"\n",
    "question = \"Is parity conserved in weak nuclear interactions?\"\n",
    "\n",
    "prompt = build_qualitative_prompt(question, form=\"V0_empty\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "7494db0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respond with exactly one of the following: Y (yes), N (no), or X (I don't know).\n",
      "Scoring: +1 for a correct answer, -9 for an incorrect answer, 0 for responding X.\n",
      "Only answer with Y or N if you are more than 90% confident. Otherwise, respond X.\n",
      "\n",
      "Question: Is parity conserved in weak nuclear interactions?\n",
      "Answer:\n",
      "\n",
      "{'Y': 1.0, 'N': 0.00020313262939453125, 'X': 5.14984130859375e-05}\n"
     ]
    }
   ],
   "source": [
    "prompt = build_qualitative_prompt(question, form=\"V0_quant\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "f5a6eb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 1.0, 'N': 0.00070953369140625, 'X': 0.00102996826171875}\n"
     ]
    }
   ],
   "source": [
    "prompt = build_qualitative_prompt(question, form=\"V0_kitten\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "3694cb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 0.6484375, 'N': 0.07763671875, 'X': 0.271484375}\n"
     ]
    }
   ],
   "source": [
    "from abstainer.src.eval_utils import *\n",
    "\n",
    "prompt = build_qualitative_prompt(question, form=\"V0_kitten2\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f7092eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 1.0, 'N': 0.00020313262939453125, 'X': 4.267692565917969e-05}\n"
     ]
    }
   ],
   "source": [
    "from abstainer.src.eval_utils import *\n",
    "\n",
    "prompt = build_qualitative_prompt(question, form=\"V0_llm\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "6bb0a18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 0.0849609375, 'N': 0.00176239013671875, 'X': 0.9140625}\n"
     ]
    }
   ],
   "source": [
    "from abstainer.src.eval_utils import *\n",
    "\n",
    "prompt = build_qualitative_prompt(question, form=\"V0_love\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "90e0019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 0.953125, 'N': 0.032470703125, 'X': 0.015380859375}\n"
     ]
    }
   ],
   "source": [
    "from abstainer.src.eval_utils import *\n",
    "\n",
    "prompt = build_qualitative_prompt(question, form=\"V0_hate\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3df1d23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "12b101a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Y': 1.0, 'N': 0.0001583099365234375, 'X': 0.000335693359375}\n"
     ]
    }
   ],
   "source": [
    "from abstainer.src.eval_utils import *\n",
    "\n",
    "prompt = build_qualitative_prompt(question, form=\"V0_I\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6cd7067",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ID', 'K']\n",
      "[1735, 236855]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.tokenize(\"IDK\"))                # shows how it splits (e.g., ['ID', 'K'])\n",
    "print(tokenizer.encode(\"IDK\", add_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1636c32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': {'Y': 236874, 'N': 236797, 'IDK': 1735}, 'logits': {'Y': 28.875, 'N': 18.625, 'IDK': 14.3125}, 'probs': {'Y': 0.9921875, 'N': 3.504753112792969e-05, 'IDK': 4.6938657760620117e-07}, 'probs_norm': {'Y': 0.9999642047028743, 'N': 3.532223152492794e-05, 'IDK': 4.730656007802849e-07}}\n"
     ]
    }
   ],
   "source": [
    "prompt = build_qualitative_prompt(question, form=\"V0_unless\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5bf7860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': {'Y': 236874, 'N': 236797, 'IDK': 1735}, 'logits': {'Y': 24.375, 'N': 18.375, 'IDK': 17.625}, 'probs': {'Y': 0.86328125, 'N': 0.00213623046875, 'IDK': 0.001007080078125}, 'probs_norm': {'Y': 0.9963720897467507, 'N': 0.002465570075023775, 'IDK': 0.001162340178225494}}\n"
     ]
    }
   ],
   "source": [
    "prompt = build_qualitative_prompt(question, form=\"V0_all\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fddbc8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': {'Y': 236874, 'N': 236797, 'IDK': 1735}, 'logits': {'Y': 14.3125, 'N': 14.375, 'IDK': 22.375}, 'probs': {'Y': 0.0002956390380859375, 'N': 0.0003147125244140625, 'IDK': 0.9375}, 'probs_norm': {'Y': 0.0003151431359791802, 'N': 0.00033547495120364346, 'IDK': 0.9993493819128172}}\n"
     ]
    }
   ],
   "source": [
    "prompt = build_qualitative_prompt(question, form=\"V0_order\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71e6f3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': {'Y': 236874, 'N': 236797, 'IDK': 1735}, 'logits': {'Y': 25.75, 'N': 20.25, 'IDK': 29.875}, 'probs': {'Y': 0.015869140625, 'N': 6.4849853515625e-05, 'IDK': 0.98046875}, 'probs_norm': {'Y': 0.015926432134639606, 'N': 6.508397747328686e-05, 'IDK': 0.9840084838878871}}\n"
     ]
    }
   ],
   "source": [
    "prompt = build_qualitative_prompt(question, form=\"V0_saved\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eabba8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': {'Y': 236874, 'N': 236797, 'IDK': 1735}, 'logits': {'Y': 25.75, 'N': 19.875, 'IDK': 18.5}, 'probs': {'Y': 0.95703125, 'N': 0.002685546875, 'IDK': 0.00067901611328125}, 'probs_norm': {'Y': 0.9964966913195796, 'N': 0.002796291735845759, 'IDK': 0.000707016944574638}}\n"
     ]
    }
   ],
   "source": [
    "prompt = build_qualitative_prompt(question, form=\"V0_destroyed\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e83c809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': {'Y': 236874, 'N': 236797, 'IDK': 1735}, 'logits': {'Y': 30.125, 'N': 19.875, 'IDK': 15.5}, 'probs': {'Y': 1.0, 'N': 3.528594970703125e-05, 'IDK': 4.4517219066619873e-07}, 'probs_norm': {'Y': 0.9999642701547697, 'N': 3.5284688945509416e-05, 'IDK': 4.4515628473272544e-07}}\n"
     ]
    }
   ],
   "source": [
    "prompt = build_qualitative_prompt(question, form=\"V0_happy\")\n",
    "print(run_yesno_probe(tokenizer, model, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbdf531",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
