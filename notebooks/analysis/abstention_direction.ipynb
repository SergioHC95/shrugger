{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f1daa05",
   "metadata": {},
   "source": [
    "## Abstention Direction Analysis\n",
    "\n",
    "In this section, we'll identify a one-dimensional activation direction in the residual stream that linearly separates abstention from non-abstention at the answer token, focusing on forms V1 and V2 which elicit abstention more reliably.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "696557d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2811d213",
   "metadata": {},
   "source": [
    "### 1. Load and Prepare Data\n",
    "\n",
    "First, we'll load the residual stream data for forms V1 and V2, and filter based on CA scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44fe162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading residual stream data from by_layer directory...\n",
      "Found 35 layer directories\n",
      "Processing layer 0...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 1...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 2...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 3...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 4...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 5...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 6...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 7...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 8...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 9...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 10...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 11...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 12...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 13...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 14...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 15...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 16...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 17...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 18...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 19...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 20...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 21...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 22...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 23...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 24...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 25...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 26...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 27...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 28...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 29...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 30...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 31...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 32...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 33...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Processing layer 34...\n",
      "  Loaded V1_num_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p4 with 1050 questions\n",
      "  Loaded V1_alpha_p3 with 1050 questions\n",
      "  Loaded V1_num_p2 with 1050 questions\n",
      "  Loaded V1_num_p1 with 1050 questions\n",
      "  Loaded V1_num_p3 with 1050 questions\n",
      "  Loaded V1_alpha_p1 with 1050 questions\n",
      "  Loaded V1_alpha_p5 with 1050 questions\n",
      "  Loaded V1_alpha_p2 with 1050 questions\n",
      "  Loaded V1_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p4 with 1050 questions\n",
      "  Loaded V2_num_p5 with 1050 questions\n",
      "  Loaded V2_alpha_p3 with 1050 questions\n",
      "  Loaded V2_num_p3 with 1050 questions\n",
      "  Loaded V2_alpha_p1 with 1050 questions\n",
      "  Loaded V2_alpha_p2 with 1050 questions\n",
      "  Loaded V2_alpha_p5 with 1050 questions\n",
      "  Loaded V2_num_p1 with 1050 questions\n",
      "  Loaded V2_num_p2 with 1050 questions\n",
      "  Loaded V2_num_p4 with 1050 questions\n",
      "Loaded data for 35 layers with a total of 700 experiments\n",
      "\n",
      "Layer 0 contains 20 experiments\n",
      "  Experiment: V1_num_p4\n",
      "  Number of questions: 1050\n",
      "  Vector dimension: (2560,)\n",
      "  Metadata keys: ['experiment_id', 'form', 'layer', 'num_questions', 'vector_dim', 'question_metadata']\n",
      "  First question: Is photosynthesis how plants make their own food?\n",
      "  Subject: Biology\n",
      "  Answer: Yes\n"
     ]
    }
   ],
   "source": [
    "def load_residual_stream_data(forms=['V1', 'V2'], split='train'):\n",
    "    \"\"\"\n",
    "    Load residual stream data for specified forms and split from the by_layer directory.\n",
    "    \n",
    "    Args:\n",
    "        forms: List of forms to include (default: V1 and V2)\n",
    "        split: Dataset split to use (default: train)\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping layer indices to dictionaries of experiment data\n",
    "    \"\"\"\n",
    "    import json\n",
    "    # Path to the by_layer directory\n",
    "    by_layer_dir = Path(\"/home/sergio/projects/MATS-Project/results/by_layer_20250911_112921/by_layer\")\n",
    "\n",
    "    if not by_layer_dir.exists():\n",
    "        print(f\"Directory not found: {by_layer_dir}\")\n",
    "        return {}\n",
    "\n",
    "    # Initialize data structure\n",
    "    # {layer_idx: {exp_name: {'vectors': ndarray, 'question_ids': ndarray, 'metadata': dict}}}\n",
    "    residual_data = {}\n",
    "\n",
    "    # List all layer directories\n",
    "    layer_dirs = sorted([d for d in by_layer_dir.glob(\"layer_*\") if d.is_dir()])\n",
    "    print(f\"Found {len(layer_dirs)} layer directories\")\n",
    "\n",
    "    # Process each layer directory\n",
    "    for layer_dir in layer_dirs:\n",
    "        layer_idx = int(layer_dir.name.split('_')[1])\n",
    "        print(f\"Processing layer {layer_idx}...\")\n",
    "\n",
    "        # Initialize layer data\n",
    "        residual_data[layer_idx] = {}\n",
    "\n",
    "        # Find all experiment files for the specified forms\n",
    "        for form in forms:\n",
    "            # Look for .npz files matching the form\n",
    "            npz_files = list(layer_dir.glob(f\"{form}_*_{form}.npz\"))\n",
    "\n",
    "            for npz_file in npz_files:\n",
    "                # Get the experiment name (e.g., \"V1_alpha_p1\")\n",
    "                exp_name = npz_file.stem.replace(f\"_{form}\", \"\")\n",
    "\n",
    "                # Load the npz data\n",
    "                npz_data = np.load(npz_file)\n",
    "                question_ids = npz_data['question_ids']\n",
    "                vectors = npz_data['vectors']\n",
    "\n",
    "                # Load the corresponding JSON metadata\n",
    "                json_file = npz_file.with_suffix('.json')\n",
    "                if json_file.exists():\n",
    "                    with open(json_file) as f:\n",
    "                        metadata = json.load(f)\n",
    "                else:\n",
    "                    metadata = {}\n",
    "\n",
    "                # Store the data\n",
    "                residual_data[layer_idx][exp_name] = {\n",
    "                    'vectors': vectors,\n",
    "                    'question_ids': question_ids,\n",
    "                    'metadata': metadata\n",
    "                }\n",
    "\n",
    "                print(f\"  Loaded {exp_name} with {len(question_ids)} questions\")\n",
    "\n",
    "    return residual_data\n",
    "\n",
    "# Load residual stream data for forms V1 and V2\n",
    "try:\n",
    "    print(\"Loading residual stream data from by_layer directory...\")\n",
    "    residual_data = load_residual_stream_data(forms=['V1', 'V2'])\n",
    "\n",
    "    # Count the total number of experiments across all layers\n",
    "    total_experiments = sum(len(layer_data) for layer_data in residual_data.values())\n",
    "\n",
    "    if residual_data:\n",
    "        print(f\"Loaded data for {len(residual_data)} layers with a total of {total_experiments} experiments\")\n",
    "\n",
    "        # Print details for the first layer\n",
    "        first_layer = min(residual_data.keys())\n",
    "        first_layer_data = residual_data[first_layer]\n",
    "        print(f\"\\nLayer {first_layer} contains {len(first_layer_data)} experiments\")\n",
    "\n",
    "        # Print details for the first experiment in the first layer\n",
    "        if first_layer_data:\n",
    "            first_exp = next(iter(first_layer_data))\n",
    "            exp_data = first_layer_data[first_exp]\n",
    "            print(f\"  Experiment: {first_exp}\")\n",
    "            print(f\"  Number of questions: {len(exp_data['question_ids'])}\")\n",
    "            print(f\"  Vector dimension: {exp_data['vectors'][0].shape}\")\n",
    "\n",
    "            # Print metadata keys\n",
    "            if 'metadata' in exp_data and exp_data['metadata']:\n",
    "                print(f\"  Metadata keys: {list(exp_data['metadata'].keys())}\")\n",
    "\n",
    "                # Print question metadata for the first question if available\n",
    "                if 'question_metadata' in exp_data['metadata']:\n",
    "                    first_q_id = exp_data['question_ids'][0]\n",
    "                    if first_q_id in exp_data['metadata']['question_metadata']:\n",
    "                        q_meta = exp_data['metadata']['question_metadata'][first_q_id]\n",
    "                        print(f\"  First question: {q_meta.get('question', 'N/A')}\")\n",
    "                        print(f\"  Subject: {q_meta.get('subject', 'N/A')}\")\n",
    "                        print(f\"  Answer: {q_meta.get('answer', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"No residual stream data found for forms V1 and V2\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading residual stream data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c06e025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 layer directories\n",
      "\n",
      "Vector counts per layer:\n",
      "Layer 0: 63000 vectors\n",
      "Layer 1: 63000 vectors\n",
      "Layer 2: 63000 vectors\n",
      "Layer 3: 63000 vectors\n",
      "Layer 4: 63000 vectors\n",
      "Layer 5: 63000 vectors\n",
      "Layer 6: 63000 vectors\n",
      "Layer 7: 63000 vectors\n",
      "Layer 8: 63000 vectors\n",
      "Layer 9: 63000 vectors\n",
      "Layer 10: 63000 vectors\n",
      "Layer 11: 63000 vectors\n",
      "Layer 12: 63000 vectors\n",
      "Layer 13: 63000 vectors\n",
      "Layer 14: 63000 vectors\n",
      "Layer 15: 63000 vectors\n",
      "Layer 16: 63000 vectors\n",
      "Layer 17: 63000 vectors\n",
      "Layer 18: 63000 vectors\n",
      "Layer 19: 63000 vectors\n",
      "Layer 20: 63000 vectors\n",
      "Layer 21: 63000 vectors\n",
      "Layer 22: 63000 vectors\n",
      "Layer 23: 63000 vectors\n",
      "Layer 24: 63000 vectors\n",
      "Layer 25: 63000 vectors\n",
      "Layer 26: 63000 vectors\n",
      "Layer 27: 63000 vectors\n",
      "Layer 28: 63000 vectors\n",
      "Layer 29: 63000 vectors\n",
      "Layer 30: 63000 vectors\n",
      "Layer 31: 63000 vectors\n",
      "Layer 32: 63000 vectors\n",
      "Layer 33: 63000 vectors\n",
      "Layer 34: 63000 vectors\n",
      "\n",
      "All layers have the same number of vectors: 63000\n",
      "\n",
      "Vector counts for V1 and V2 experiments (first layer only):\n",
      "V1_alpha_p1_V1: 1050 vectors\n",
      "V1_alpha_p2_V1: 1050 vectors\n",
      "V1_alpha_p3_V1: 1050 vectors\n",
      "V1_alpha_p4_V1: 1050 vectors\n",
      "V1_alpha_p5_V1: 1050 vectors\n",
      "V1_num_p1_V1: 1050 vectors\n",
      "V1_num_p2_V1: 1050 vectors\n",
      "V1_num_p3_V1: 1050 vectors\n",
      "V1_num_p4_V1: 1050 vectors\n",
      "V1_num_p5_V1: 1050 vectors\n",
      "V2_alpha_p1_V2: 1050 vectors\n",
      "V2_alpha_p2_V2: 1050 vectors\n",
      "V2_alpha_p3_V2: 1050 vectors\n",
      "V2_alpha_p4_V2: 1050 vectors\n",
      "V2_alpha_p5_V2: 1050 vectors\n",
      "V2_num_p1_V2: 1050 vectors\n",
      "V2_num_p2_V2: 1050 vectors\n",
      "V2_num_p3_V2: 1050 vectors\n",
      "V2_num_p4_V2: 1050 vectors\n",
      "V2_num_p5_V2: 1050 vectors\n",
      "\n",
      "Total V1: 10500 vectors\n",
      "Total V2: 10500 vectors\n",
      "Combined V1+V2: 21000 vectors\n"
     ]
    }
   ],
   "source": [
    "# Check vector counts in each layer file\n",
    "try:\n",
    "    from pathlib import Path\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    # Path to the by_layer directory\n",
    "    by_layer_dir = Path(\"/home/sergio/projects/MATS-Project/results/by_layer_20250911_112921/by_layer\")\n",
    "\n",
    "    if not by_layer_dir.exists():\n",
    "        print(f\"Directory not found: {by_layer_dir}\")\n",
    "    else:\n",
    "        # Get all layer directories\n",
    "        layer_dirs = sorted([d for d in by_layer_dir.glob(\"layer_*\") if d.is_dir()])\n",
    "        print(f\"Found {len(layer_dirs)} layer directories\")\n",
    "\n",
    "        # Track total vectors per layer and by experiment type\n",
    "        layer_counts = {}\n",
    "        experiment_counts = {}\n",
    "\n",
    "        # Process each layer directory\n",
    "        for layer_dir in layer_dirs:\n",
    "            layer_idx = int(layer_dir.name.split('_')[1])\n",
    "            layer_counts[layer_idx] = 0\n",
    "\n",
    "            # Get all npz files in this layer\n",
    "            npz_files = list(layer_dir.glob(\"*.npz\"))\n",
    "\n",
    "            # Process each npz file\n",
    "            for npz_file in npz_files:\n",
    "                exp_name = npz_file.stem\n",
    "\n",
    "                # Load the npz data\n",
    "                try:\n",
    "                    data = np.load(npz_file)\n",
    "                    if 'question_ids' in data and 'vectors' in data:\n",
    "                        num_vectors = len(data['question_ids'])\n",
    "                        layer_counts[layer_idx] += num_vectors\n",
    "\n",
    "                        # Track by experiment type\n",
    "                        if exp_name not in experiment_counts:\n",
    "                            experiment_counts[exp_name] = {}\n",
    "                        experiment_counts[exp_name][layer_idx] = num_vectors\n",
    "                    else:\n",
    "                        print(f\"  Warning: Missing expected keys in {npz_file}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error loading {npz_file}: {e}\")\n",
    "\n",
    "        # Print summary of vector counts per layer\n",
    "        print(\"\\nVector counts per layer:\")\n",
    "        for layer_idx in sorted(layer_counts.keys()):\n",
    "            print(f\"Layer {layer_idx}: {layer_counts[layer_idx]} vectors\")\n",
    "\n",
    "        # Check if all layers have the same count\n",
    "        counts = list(layer_counts.values())\n",
    "        if len(set(counts)) == 1:\n",
    "            print(f\"\\nAll layers have the same number of vectors: {counts[0]}\")\n",
    "        else:\n",
    "            print(f\"\\nLayers have different vector counts. Min: {min(counts)}, Max: {max(counts)}\")\n",
    "\n",
    "        # Print counts for V1 and V2 experiments\n",
    "        print(\"\\nVector counts for V1 and V2 experiments (first layer only):\")\n",
    "        first_layer = min(layer_counts.keys())\n",
    "        v1v2_experiments = {name: counts[first_layer] for name, counts in experiment_counts.items()\n",
    "                           if name.startswith('V1') or name.startswith('V2')}\n",
    "\n",
    "        for exp_name, count in sorted(v1v2_experiments.items()):\n",
    "            print(f\"{exp_name}: {count} vectors\")\n",
    "\n",
    "        # Calculate total for V1 and V2\n",
    "        v1_total = sum(count for name, count in v1v2_experiments.items() if name.startswith('V1'))\n",
    "        v2_total = sum(count for name, count in v1v2_experiments.items() if name.startswith('V2'))\n",
    "        print(f\"\\nTotal V1: {v1_total} vectors\")\n",
    "        print(f\"Total V2: {v2_total} vectors\")\n",
    "        print(f\"Combined V1+V2: {v1_total + v2_total} vectors\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error checking vector counts: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93fdd9",
   "metadata": {},
   "source": [
    "### 3. Balance the Dataset\n",
    "\n",
    "We'll balance the positive and negative classes across subject areas and prompt labels to avoid confounds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad537126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 runs with metrics data:\n",
      "  - run_20250911_062422\n",
      "\n",
      "Using latest run: run_20250911_062422\n",
      "Loaded 63000 metric records\n",
      "\n",
      "Columns in metrics_df:\n",
      "['run', 'experiment', 'form', 'label_type', 'permutation', 'id', 'question', 'answer', 'subject', 'difficulty', 'split', 'pred_label', 'canonical_label', 'score', 'ca_score', 'hedge_score', 'canonical_probs', 'canonical_probs_norm']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the metrics directory\n",
    "metrics_dir = Path('./results/metrics_analysis')\n",
    "\n",
    "# Find all run directories\n",
    "run_dirs = [d for d in metrics_dir.iterdir() if d.is_dir() and d.name.startswith('run_')]\n",
    "\n",
    "if not run_dirs:\n",
    "    print(\"No metrics data found. Please run the run_metrics_analysis.py script first.\")\n",
    "else:\n",
    "    print(f\"Found {len(run_dirs)} runs with metrics data:\")\n",
    "    for d in run_dirs:\n",
    "        print(f\"  - {d.name}\")\n",
    "\n",
    "    # Use the most recent run by default\n",
    "    latest_run = sorted(run_dirs)[-1]\n",
    "    print(f\"\\nUsing latest run: {latest_run.name}\")\n",
    "\n",
    "# Load the all_metrics.csv file from the latest run\n",
    "try:\n",
    "    metrics_file = latest_run / 'all_metrics.csv'\n",
    "    metrics_df = pd.read_csv(metrics_file)\n",
    "    print(f\"Loaded {len(metrics_df)} metric records\")\n",
    "\n",
    "    # Display the first few rows and column names\n",
    "    print(\"\\nColumns in metrics_df:\")\n",
    "    print(metrics_df.columns.tolist())\n",
    "    metrics_df.head()\n",
    "except NameError:\n",
    "    print(\"No metrics data available. Please run the run_metrics_analysis.py script first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2acfdb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16800 examples for forms V1 and V2 in the train split\n",
      "CA score quantiles - 25%: 0.0005, 75%: 0.4686\n",
      "Positive class (high CA): 4200 examples\n",
      "Negative class (low CA): 4201 examples\n",
      "\n",
      "Distribution across forms:\n",
      "      Positive  Negative\n",
      "form                    \n",
      "V1        2298      1965\n",
      "V2        1902      2236\n",
      "\n",
      "Distribution across subjects (top 5):\n",
      "             Positive  Negative\n",
      "subject                        \n",
      "Biology         413.0       NaN\n",
      "Psychology      390.0       NaN\n",
      "Physics         386.0     319.0\n",
      "Medicine        383.0       NaN\n",
      "History         335.0       NaN\n",
      "Engineering       NaN     424.0\n",
      "Chemistry         NaN     384.0\n",
      "Computing         NaN     351.0\n",
      "Earth             NaN     333.0\n",
      "\n",
      "Distribution across label types:\n",
      "            Positive  Negative\n",
      "label_type                    \n",
      "num             2168      1689\n",
      "alpha           2032      2512\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Filter metrics_df for forms V1 and V2 and split 'train'\n",
    "    v1v2_metrics = metrics_df[(metrics_df['form'].isin(['V1', 'V2'])) &\n",
    "                             (metrics_df['split'] == 'train')]\n",
    "\n",
    "    print(f\"Found {len(v1v2_metrics)} examples for forms V1 and V2 in the train split\")\n",
    "\n",
    "    # Calculate the 25% and 75% quantiles of CA scores\n",
    "    q25 = v1v2_metrics['ca_score'].quantile(0.25)\n",
    "    q75 = v1v2_metrics['ca_score'].quantile(0.75)\n",
    "\n",
    "    print(f\"CA score quantiles - 25%: {q25:.4f}, 75%: {q75:.4f}\")\n",
    "\n",
    "    # Create positive and negative classes based on CA scores\n",
    "    positive_class = v1v2_metrics[v1v2_metrics['ca_score'] >= q75]\n",
    "    negative_class = v1v2_metrics[v1v2_metrics['ca_score'] <= q25]\n",
    "\n",
    "    print(f\"Positive class (high CA): {len(positive_class)} examples\")\n",
    "    print(f\"Negative class (low CA): {len(negative_class)} examples\")\n",
    "\n",
    "    # Check distribution across subject areas and prompt forms\n",
    "    print(\"\\nDistribution across forms:\")\n",
    "    print(pd.concat([\n",
    "        positive_class['form'].value_counts().rename('Positive'),\n",
    "        negative_class['form'].value_counts().rename('Negative')\n",
    "    ], axis=1))\n",
    "\n",
    "    print(\"\\nDistribution across subjects (top 5):\")\n",
    "    print(pd.concat([\n",
    "        positive_class['subject'].value_counts().head().rename('Positive'),\n",
    "        negative_class['subject'].value_counts().head().rename('Negative')\n",
    "    ], axis=1))\n",
    "\n",
    "    print(\"\\nDistribution across label types:\")\n",
    "    print(pd.concat([\n",
    "        positive_class['label_type'].value_counts().rename('Positive'),\n",
    "        negative_class['label_type'].value_counts().rename('Negative')\n",
    "    ], axis=1))\n",
    "\n",
    "except NameError:\n",
    "    print(\"metrics_df not available. Please run the previous cells to load the metrics data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98bf2d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced positive class: 3388 examples\n",
      "Balanced negative class: 3388 examples\n",
      "\n",
      "Balanced distribution across forms:\n",
      "      Positive  Negative\n",
      "form                    \n",
      "V1        1759      1759\n",
      "V2        1629      1629\n",
      "\n",
      "Balanced distribution across label types:\n",
      "            Positive  Negative\n",
      "label_type                    \n",
      "alpha           1889      1889\n",
      "num             1499      1499\n",
      "\n",
      "Final balanced dataset: 6776 examples\n"
     ]
    }
   ],
   "source": [
    "def balance_dataset(positive_df, negative_df, balance_cols=['subject', 'form', 'label_type']):\n",
    "    \"\"\"\n",
    "    Balance positive and negative classes across specified columns.\n",
    "    \n",
    "    Args:\n",
    "        positive_df: DataFrame containing positive class examples\n",
    "        negative_df: DataFrame containing negative class examples\n",
    "        balance_cols: List of columns to balance across\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (balanced_positive_df, balanced_negative_df)\n",
    "    \"\"\"\n",
    "    # Create a combined groupby across all balance columns\n",
    "    pos_counts = positive_df.groupby(balance_cols).size()\n",
    "    neg_counts = negative_df.groupby(balance_cols).size()\n",
    "\n",
    "    # Find common groups\n",
    "    common_groups = set(pos_counts.index).intersection(set(neg_counts.index))\n",
    "\n",
    "    # Initialize empty DataFrames for balanced data\n",
    "    balanced_pos = pd.DataFrame()\n",
    "    balanced_neg = pd.DataFrame()\n",
    "\n",
    "    # For each common group, take the minimum count from both classes\n",
    "    for group in common_groups:\n",
    "        # Convert group to tuple if it's not already\n",
    "        group_tuple = group if isinstance(group, tuple) else (group,)\n",
    "\n",
    "        # Create filter conditions\n",
    "        filter_conditions = {col: val for col, val in zip(balance_cols, group_tuple)}\n",
    "\n",
    "        # Filter the DataFrames\n",
    "        pos_group = positive_df\n",
    "        neg_group = negative_df\n",
    "\n",
    "        for col, val in filter_conditions.items():\n",
    "            pos_group = pos_group[pos_group[col] == val]\n",
    "            neg_group = neg_group[neg_group[col] == val]\n",
    "\n",
    "        # Get counts\n",
    "        pos_count = len(pos_group)\n",
    "        neg_count = len(neg_group)\n",
    "        min_count = min(pos_count, neg_count)\n",
    "\n",
    "        if min_count > 0:\n",
    "            # Sample min_count examples from each group\n",
    "            balanced_pos = pd.concat([balanced_pos, pos_group.sample(min_count, random_state=42)])\n",
    "            balanced_neg = pd.concat([balanced_neg, neg_group.sample(min_count, random_state=42)])\n",
    "\n",
    "    return balanced_pos, balanced_neg\n",
    "\n",
    "try:\n",
    "    # Balance the dataset\n",
    "    balanced_positive, balanced_negative = balance_dataset(positive_class, negative_class)\n",
    "\n",
    "    print(f\"Balanced positive class: {len(balanced_positive)} examples\")\n",
    "    print(f\"Balanced negative class: {len(balanced_negative)} examples\")\n",
    "\n",
    "    # Check the balanced distribution\n",
    "    print(\"\\nBalanced distribution across forms:\")\n",
    "    print(pd.concat([\n",
    "        balanced_positive['form'].value_counts().rename('Positive'),\n",
    "        balanced_negative['form'].value_counts().rename('Negative')\n",
    "    ], axis=1))\n",
    "\n",
    "    print(\"\\nBalanced distribution across label types:\")\n",
    "    print(pd.concat([\n",
    "        balanced_positive['label_type'].value_counts().rename('Positive'),\n",
    "        balanced_negative['label_type'].value_counts().rename('Negative')\n",
    "    ], axis=1))\n",
    "\n",
    "    # Create a combined dataset with labels\n",
    "    balanced_positive['class'] = 1  # High CA (abstention)\n",
    "    balanced_negative['class'] = 0  # Low CA (non-abstention)\n",
    "\n",
    "    balanced_dataset = pd.concat([balanced_positive, balanced_negative])\n",
    "    print(f\"\\nFinal balanced dataset: {len(balanced_dataset)} examples\")\n",
    "\n",
    "    # Shuffle the dataset\n",
    "    balanced_dataset = balanced_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "except NameError:\n",
    "    print(\"positive_class or negative_class not available. Please run the previous cells.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3461653c",
   "metadata": {},
   "source": [
    "### 4. Extract Residual Stream Vectors\n",
    "\n",
    "Now we'll extract the residual stream vectors for each example in our balanced dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "996cb28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing residual vectors layer by layer to save memory...\n",
      "Found 30 layers with existing results: [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "Processing 5 out of 35 layers in reverse order...\n",
      "Processing layer 4 (1/5)...\n",
      "  Layer 4: 6855 positive examples, 6785 negative examples\n",
      "  Saved data for layer 4 to results/abstention_direction/residual_vectors_20250913_012645_layer_4.pkl\n",
      "Processing layer 3 (2/5)...\n",
      "  Layer 3: 6855 positive examples, 6785 negative examples\n",
      "  Saved data for layer 3 to results/abstention_direction/residual_vectors_20250913_012645_layer_3.pkl\n",
      "Processing layer 2 (3/5)...\n",
      "  Layer 2: 6855 positive examples, 6785 negative examples\n",
      "  Saved data for layer 2 to results/abstention_direction/residual_vectors_20250913_012645_layer_2.pkl\n",
      "Processing layer 1 (4/5)...\n",
      "  Layer 1: 6855 positive examples, 6785 negative examples\n",
      "  Saved data for layer 1 to results/abstention_direction/residual_vectors_20250913_012645_layer_1.pkl\n",
      "Processing layer 0 (5/5)...\n",
      "  Layer 0: 6855 positive examples, 6785 negative examples\n",
      "  Saved data for layer 0 to results/abstention_direction/residual_vectors_20250913_012645_layer_0.pkl\n",
      "\n",
      "Sample layer 0 details:\n",
      "  Positive examples: (6855, 2560)\n",
      "  Negative examples: (6785, 2560)\n",
      "  Vector dimension: (2560,)\n",
      "\n",
      "Processing complete. Data saved layer by layer to avoid memory issues.\n",
      "Total layers processed: 5\n",
      "Total layers available: 35\n",
      "\n",
      "To load all layers for analysis, use:\n",
      "residual_vectors = load_all_residual_vectors('./results/abstention_direction')\n"
     ]
    }
   ],
   "source": [
    "def extract_residual_vectors(dataset, residual_data):\n",
    "    \"\"\"\n",
    "    Extract residual stream vectors for examples in the dataset.\n",
    "    Skip layers that already have saved results.\n",
    "    \n",
    "    Args:\n",
    "        dataset: DataFrame containing examples with question IDs, form, and label_type\n",
    "        residual_data: Dictionary mapping layer indices to dictionaries of experiment data\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping layer indices to matrices H_pos and H_neg\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import pickle\n",
    "    from datetime import datetime\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    save_dir = Path(\"./results/abstention_direction\")\n",
    "    save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create timestamp for filenames\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Check for existing layer files\n",
    "    existing_layer_files = glob.glob(str(save_dir / \"residual_vectors_*_layer_*.pkl\"))\n",
    "    existing_layers = set()\n",
    "    for file_path in existing_layer_files:\n",
    "        try:\n",
    "            layer_part = file_path.split(\"_layer_\")[1]\n",
    "            layer_num = int(layer_part.split(\".pkl\")[0])\n",
    "            existing_layers.add(layer_num)\n",
    "        except (IndexError, ValueError):\n",
    "            continue\n",
    "\n",
    "    print(f\"Found {len(existing_layers)} layers with existing results: {sorted(existing_layers)}\")\n",
    "\n",
    "    # Initialize dictionaries to store residual vectors for each layer\n",
    "    result = {}\n",
    "\n",
    "    # Sort layer indices in reverse order (start from last layer)\n",
    "    layer_indices = sorted(residual_data.keys(), reverse=True)\n",
    "    layers_to_process = [layer for layer in layer_indices if layer not in existing_layers]\n",
    "    total_layers = len(layer_indices)\n",
    "    print(f\"Processing {len(layers_to_process)} out of {total_layers} layers in reverse order...\")\n",
    "\n",
    "    # Process each layer\n",
    "    for i, layer_idx in enumerate(layers_to_process):\n",
    "        layer_data = residual_data[layer_idx]\n",
    "        print(f\"Extracting vectors for layer {layer_idx} ({i+1}/{len(layers_to_process)})...\")\n",
    "\n",
    "        # Initialize positive and negative vectors for this layer\n",
    "        H_pos = []  # For high CA examples (class 1)\n",
    "        H_neg = []  # For low CA examples (class 0)\n",
    "        pos_question_ids = []\n",
    "        neg_question_ids = []\n",
    "\n",
    "        # Process each experiment in this layer\n",
    "        for exp_name, exp_data in layer_data.items():\n",
    "            # Extract form and label_type from experiment name\n",
    "            parts = exp_name.split(\"_\")\n",
    "            if len(parts) < 2:\n",
    "                continue\n",
    "\n",
    "            form = parts[0]\n",
    "            label_type = parts[1]\n",
    "\n",
    "            # Get question IDs and vectors\n",
    "            question_ids = exp_data['question_ids']\n",
    "            vectors = exp_data['vectors']\n",
    "\n",
    "            # Match with our dataset\n",
    "            for i, q_id in enumerate(question_ids):\n",
    "                # Find matching rows in our dataset\n",
    "                matching_rows = dataset[(dataset['id'] == q_id) &\n",
    "                                       (dataset['form'] == form) &\n",
    "                                       (dataset['label_type'] == label_type)]\n",
    "\n",
    "                if len(matching_rows) > 0:\n",
    "                    # Get the class label (1 for positive, 0 for negative)\n",
    "                    class_label = matching_rows['class'].values[0]\n",
    "\n",
    "                    # Get the vector for this question\n",
    "                    vector = vectors[i]\n",
    "\n",
    "                    # Add to the appropriate list\n",
    "                    if class_label == 1:\n",
    "                        H_pos.append(vector)\n",
    "                        pos_question_ids.append(q_id)\n",
    "                    else:\n",
    "                        H_neg.append(vector)\n",
    "                        neg_question_ids.append(q_id)\n",
    "\n",
    "        # Convert lists to numpy arrays if they're not empty\n",
    "        if H_pos and H_neg:\n",
    "            result[layer_idx] = {\n",
    "                'positive': np.array(H_pos),\n",
    "                'negative': np.array(H_neg),\n",
    "                'pos_question_ids': pos_question_ids,\n",
    "                'neg_question_ids': neg_question_ids\n",
    "            }\n",
    "            print(f\"  Layer {layer_idx}: {len(H_pos)} positive examples, {len(H_neg)} negative examples\")\n",
    "\n",
    "            # Save data for this layer immediately\n",
    "            save_path = save_dir / f\"residual_vectors_{timestamp}_layer_{layer_idx}.pkl\"\n",
    "\n",
    "            # Prepare partial results dictionary with current data\n",
    "            partial_vectors = {\n",
    "                'positive': {layer: data['positive'] for layer, data in result.items()},\n",
    "                'negative': {layer: data['negative'] for layer, data in result.items()}\n",
    "            }\n",
    "\n",
    "            save_data = {\n",
    "                'residual_vectors': partial_vectors,\n",
    "                'residual_vectors_by_layer': result,\n",
    "                'timestamp': timestamp,\n",
    "                'completed_layers': list(result.keys()),\n",
    "                'forms': dataset['form'].unique().tolist() if 'form' in dataset.columns else ['V1', 'V2']\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                with open(save_path, 'wb') as f:\n",
    "                    pickle.dump(save_data, f)\n",
    "                print(f\"  Saved data for layer {layer_idx} to {save_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Error saving layer data: {e}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "try:\n",
    "    # Check if we have residual data and balanced dataset\n",
    "    if 'residual_data' in locals() and 'balanced_dataset' in locals():\n",
    "        print(\"Processing residual vectors layer by layer to save memory...\")\n",
    "\n",
    "        import glob\n",
    "        import pickle\n",
    "        from datetime import datetime\n",
    "        from pathlib import Path\n",
    "\n",
    "        # Create save directory if it doesn't exist\n",
    "        save_dir = Path(\"./results/abstention_direction\")\n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Create timestamp for filenames\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "        # Check for existing layer files\n",
    "        existing_layer_files = glob.glob(str(save_dir / \"residual_vectors_*_layer_*.pkl\"))\n",
    "        existing_layers = set()\n",
    "        for file_path in existing_layer_files:\n",
    "            try:\n",
    "                layer_part = file_path.split(\"_layer_\")[1]\n",
    "                layer_num = int(layer_part.split(\".pkl\")[0])\n",
    "                existing_layers.add(layer_num)\n",
    "            except (IndexError, ValueError):\n",
    "                continue\n",
    "\n",
    "        print(f\"Found {len(existing_layers)} layers with existing results: {sorted(existing_layers)}\")\n",
    "\n",
    "        # Sort layer indices in reverse order (start from last layer)\n",
    "        layer_indices = sorted(residual_data.keys(), reverse=True)\n",
    "        layers_to_process = [layer for layer in layer_indices if layer not in existing_layers]\n",
    "        total_layers = len(layer_indices)\n",
    "        print(f\"Processing {len(layers_to_process)} out of {total_layers} layers in reverse order...\")\n",
    "\n",
    "        # Track processed layers for summary\n",
    "        processed_layers = []\n",
    "\n",
    "        # Process each layer independently to save memory\n",
    "        for i, layer_idx in enumerate(layers_to_process):\n",
    "            print(f\"Processing layer {layer_idx} ({i+1}/{len(layers_to_process)})...\")\n",
    "\n",
    "            # Get layer data\n",
    "            layer_data = residual_data[layer_idx]\n",
    "\n",
    "            # Initialize positive and negative vectors for this layer\n",
    "            H_pos = []  # For high CA examples (class 1)\n",
    "            H_neg = []  # For low CA examples (class 0)\n",
    "            pos_question_ids = []\n",
    "            neg_question_ids = []\n",
    "\n",
    "            # Process each experiment in this layer\n",
    "            for exp_name, exp_data in layer_data.items():\n",
    "                # Extract form and label_type from experiment name\n",
    "                parts = exp_name.split(\"_\")\n",
    "                if len(parts) < 2:\n",
    "                    continue\n",
    "\n",
    "                form = parts[0]\n",
    "                label_type = parts[1]\n",
    "\n",
    "                # Get question IDs and vectors\n",
    "                question_ids = exp_data['question_ids']\n",
    "                vectors = exp_data['vectors']\n",
    "\n",
    "                # Match with our dataset\n",
    "                for j, q_id in enumerate(question_ids):\n",
    "                    # Find matching rows in our dataset\n",
    "                    matching_rows = balanced_dataset[(balanced_dataset['id'] == q_id) &\n",
    "                                           (balanced_dataset['form'] == form) &\n",
    "                                           (balanced_dataset['label_type'] == label_type)]\n",
    "\n",
    "                    if len(matching_rows) > 0:\n",
    "                        # Get the class label (1 for positive, 0 for negative)\n",
    "                        class_label = matching_rows['class'].values[0]\n",
    "\n",
    "                        # Get the vector for this question\n",
    "                        vector = vectors[j]\n",
    "\n",
    "                        # Add to the appropriate list\n",
    "                        if class_label == 1:\n",
    "                            H_pos.append(vector)\n",
    "                            pos_question_ids.append(q_id)\n",
    "                        else:\n",
    "                            H_neg.append(vector)\n",
    "                            neg_question_ids.append(q_id)\n",
    "\n",
    "            # Convert lists to numpy arrays if they're not empty\n",
    "            if H_pos and H_neg:\n",
    "                # Create layer result dictionary\n",
    "                layer_result = {\n",
    "                    'positive': np.array(H_pos),\n",
    "                    'negative': np.array(H_neg),\n",
    "                    'pos_question_ids': pos_question_ids,\n",
    "                    'neg_question_ids': neg_question_ids\n",
    "                }\n",
    "\n",
    "                print(f\"  Layer {layer_idx}: {len(H_pos)} positive examples, {len(H_neg)} negative examples\")\n",
    "                processed_layers.append(layer_idx)\n",
    "\n",
    "                # Save data for this layer immediately\n",
    "                save_path = save_dir / f\"residual_vectors_{timestamp}_layer_{layer_idx}.pkl\"\n",
    "\n",
    "                # Prepare save data for just this layer\n",
    "                save_data = {\n",
    "                    'residual_vectors_by_layer': {layer_idx: layer_result},\n",
    "                    'timestamp': timestamp,\n",
    "                    'layer': layer_idx,\n",
    "                    'forms': balanced_dataset['form'].unique().tolist() if 'form' in balanced_dataset.columns else ['V1', 'V2']\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        pickle.dump(save_data, f)\n",
    "                    print(f\"  Saved data for layer {layer_idx} to {save_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error saving layer data: {e}\")\n",
    "\n",
    "                # Clear memory\n",
    "                del layer_result\n",
    "                del H_pos\n",
    "                del H_neg\n",
    "                del pos_question_ids\n",
    "                del neg_question_ids\n",
    "\n",
    "        # After processing all layers, load a sample layer to demonstrate it worked\n",
    "        if processed_layers or existing_layers:\n",
    "            all_available_layers = processed_layers + list(existing_layers)\n",
    "            if all_available_layers:\n",
    "                sample_layer = min(all_available_layers)\n",
    "                sample_files = glob.glob(str(save_dir / f\"residual_vectors_*_layer_{sample_layer}.pkl\"))\n",
    "\n",
    "                if sample_files:\n",
    "                    try:\n",
    "                        with open(sample_files[0], 'rb') as f:\n",
    "                            sample_data = pickle.load(f)\n",
    "                            layer_data = sample_data['residual_vectors_by_layer'][sample_layer]\n",
    "\n",
    "                            print(f\"\\nSample layer {sample_layer} details:\")\n",
    "                            print(f\"  Positive examples: {layer_data['positive'].shape}\")\n",
    "                            print(f\"  Negative examples: {layer_data['negative'].shape}\")\n",
    "                            print(f\"  Vector dimension: {layer_data['positive'][0].shape}\")\n",
    "                            print(\"\\nProcessing complete. Data saved layer by layer to avoid memory issues.\")\n",
    "                            print(f\"Total layers processed: {len(processed_layers)}\")\n",
    "                            print(f\"Total layers available: {len(all_available_layers)}\")\n",
    "                            print(\"\\nTo load all layers for analysis, use:\")\n",
    "                            print(\"residual_vectors = load_all_residual_vectors('./results/abstention_direction')\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading sample layer: {e}\")\n",
    "            else:\n",
    "                print(\"No layers were processed or found.\")\n",
    "        else:\n",
    "            print(\"No matching vectors found in the residual data\")\n",
    "    else:\n",
    "        print(\"Residual data or balanced dataset not available\")\n",
    "except Exception as e:\n",
    "    print(f\"Error processing residual vectors: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d15f43fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_all_residual_vectors(directory_path):\n",
    "    \"\"\"\n",
    "    Load all saved residual vectors from the specified directory.\n",
    "    \n",
    "    Args:\n",
    "        directory_path: Path to the directory containing saved residual vector files\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with 'positive' and 'negative' keys, each containing layer-indexed vectors\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import pickle\n",
    "    from pathlib import Path\n",
    "\n",
    "    # Convert to Path object if it's a string\n",
    "    directory = Path(directory_path)\n",
    "\n",
    "    # Find all layer files\n",
    "    layer_files = glob.glob(str(directory / \"residual_vectors_*_layer_*.pkl\"))\n",
    "\n",
    "    # Initialize result dictionaries\n",
    "    result = {'positive': {}, 'negative': {}}\n",
    "    loaded_layers = []\n",
    "\n",
    "    print(f\"Found {len(layer_files)} layer files\")\n",
    "\n",
    "    # Load each file\n",
    "    for file_path in sorted(layer_files):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "\n",
    "                # Extract layer data\n",
    "                if 'residual_vectors_by_layer' in data:\n",
    "                    for layer, layer_data in data['residual_vectors_by_layer'].items():\n",
    "                        if 'positive' in layer_data and 'negative' in layer_data:\n",
    "                            result['positive'][layer] = layer_data['positive']\n",
    "                            result['negative'][layer] = layer_data['negative']\n",
    "                            loaded_layers.append(layer)\n",
    "                            print(f\"Loaded layer {layer} from {Path(file_path).name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "\n",
    "    print(f\"Successfully loaded data for {len(loaded_layers)} layers: {sorted(loaded_layers)}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbb86ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# After processing all layers, you can load them for analysis with:\n",
    "# residual_vectors = load_all_residual_vectors('./results/abstention_direction')\n",
    "#\n",
    "# This will give you the same structure as before, but without memory issues:\n",
    "# residual_vectors = {\n",
    "#     'positive': {layer_idx: positive_vectors_array, ...},\n",
    "#     'negative': {layer_idx: negative_vectors_array, ...}\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b224601b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c31f647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15464f03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4cbdec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f66bab1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06d7cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106c09ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6049fc4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d5603",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f776e4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759b1d13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015fb392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaea7ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78fcd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f52b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d8fab9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fec8946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fa5014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9bb939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d05612b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094360b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be26703",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba01ce09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c87b533",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26361469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3e2c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9e823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d261d7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1424ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ba49d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
