{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Accuracy Analysis\n",
    "\n",
    "This notebook analyzes model accuracy statistics across experiments by difficulty and question type (yes/no/unanswerable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n",
    "\n",
    "First, let's import necessary libraries and load the experiment data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Import our refactored functions\n",
    "from abstainer.src.analysis.experiment_accuracy import (\n",
    "    ExperimentAnalyzer,\n",
    "    find_experiment_dirs,\n",
    "    process_experiment_results\n",
    ")\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Define paths\n",
    "RESULTS_DIR = \"../../results\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all experiment directories\n",
    "experiment_dirs = find_experiment_dirs(RESULTS_DIR)\n",
    "print(f\"Found {len(experiment_dirs)} experiment directories\")\n",
    "for i, exp_dir in enumerate(experiment_dirs[:5]):\n",
    "    print(f\"  {i+1}. {exp_dir}\")\n",
    "if len(experiment_dirs) > 5:\n",
    "    print(f\"  ... and {len(experiment_dirs) - 5} more\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Process Experiment Results\n",
    "\n",
    "Now let's load the results from all experiments and organize them into a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ExperimentAnalyzer instance and load all experiments\n",
    "analyzer = ExperimentAnalyzer(RESULTS_DIR)\n",
    "results_df = analyzer.load_experiments()\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Loaded {len(results_df)} question results from {len(experiment_dirs)} experiments\")\n",
    "print(\"\\nDataFrame columns:\")\n",
    "print(results_df.columns.tolist())\n",
    "print(\"\\nSample data:\")\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze Accuracy by Difficulty and Question Type\n",
    "\n",
    "Now let's analyze the model's accuracy by difficulty level and question type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy statistics\n",
    "stats = analyzer.calculate_statistics()\n",
    "\n",
    "print(\"Overall accuracy:\", stats['overall_accuracy'])\n",
    "print(\"\\nAccuracy by difficulty:\")\n",
    "print(stats['accuracy_by_difficulty'])\n",
    "print(\"\\nAccuracy by difficulty and question type:\")\n",
    "print(stats['accuracy_by_difficulty_type'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization\n",
    "\n",
    "Let's create visualizations to better understand the accuracy patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy by difficulty\n",
    "from abstainer.src.analysis.experiment_accuracy import plot_accuracy_by_difficulty\n",
    "\n",
    "fig, ax = plot_accuracy_by_difficulty(results_df)\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy by question type and difficulty\n",
    "from abstainer.src.analysis.experiment_accuracy import plot_accuracy_by_question_type_and_difficulty\n",
    "\n",
    "fig, ax = plot_accuracy_by_question_type_and_difficulty(results_df)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Summary\n",
    "\n",
    "Let's generate a concise text summary of the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate performance summary\n",
    "summary = analyzer.generate_summary()\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Additional Analysis: Performance Across Different Forms\n",
    "\n",
    "Let's also analyze how performance varies across different prompt forms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy by form\n",
    "from abstainer.src.analysis.experiment_accuracy import plot_accuracy_by_form\n",
    "\n",
    "fig, ax = plot_accuracy_by_form(results_df)\n",
    "plt.show()\n",
    "\n",
    "# Find best and worst performing forms\n",
    "form_accuracy = results_df.groupby(['experiment_name', 'form', 'permutation'])['is_correct'].mean().reset_index()\n",
    "form_accuracy = form_accuracy.groupby('form')['is_correct'].agg(['mean', 'std', 'count']).reset_index()\n",
    "\n",
    "best_form = form_accuracy.loc[form_accuracy['mean'].idxmax()]\n",
    "worst_form = form_accuracy.loc[form_accuracy['mean'].idxmin()]\n",
    "\n",
    "print(f\"Best performing form: {best_form['form']} with {best_form['mean']:.2%} accuracy\")\n",
    "print(f\"Worst performing form: {worst_form['form']} with {worst_form['mean']:.2%} accuracy\")\n",
    "print(\"\\nAccuracy by form:\")\n",
    "print(form_accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Accuracy Variation Across Label Permutations\n",
    "\n",
    "Let's analyze how the accuracy varies across different label permutations for each form and question type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy by form and question type with permutation variation\n",
    "from abstainer.src.analysis.experiment_accuracy import plot_accuracy_by_form_and_question_type\n",
    "\n",
    "fig, ax = plot_accuracy_by_form_and_question_type(results_df)\n",
    "plt.show()\n",
    "\n",
    "# Prepare data for summary table\n",
    "if 'base_form' not in results_df.columns:\n",
    "    results_df['base_form'] = results_df['form'].astype(str).str.extract(r'(V\\d+)', expand=False)\n",
    "\n",
    "permutation_stats = (\n",
    "    results_df\n",
    "    .groupby(['base_form', 'permutation', 'question_type'], as_index=False)['is_correct']\n",
    "    .agg(mean='mean', count='count')\n",
    ")\n",
    "\n",
    "form_type_stats = (\n",
    "    permutation_stats\n",
    "    .groupby(['base_form', 'question_type'], as_index=False)['mean']\n",
    "    .agg(avg_accuracy='mean', accuracy_std='std', num_permutations='count')\n",
    ")\n",
    "\n",
    "# Display summary table\n",
    "summary_cols = ['base_form', 'question_type', 'avg_accuracy', 'accuracy_std', 'num_permutations']\n",
    "print(\"Accuracy variation across label permutations:\")\n",
    "print(form_type_stats[summary_cols].to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
