{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Abstention and Hedging Metrics\n",
    "\n",
    "This notebook provides visualizations and analysis of the Confident-Abstain (CA) and Hedging (HEDGE) metrics computed over the experimental results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Add the project root to the path to allow imports\n",
    "repo_root = Path().absolute()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "# Import the metrics analysis module\n",
    "from abstainer.src.analysis import (\n",
    "    MetricsAnalyzer,\n",
    "    load_metrics_data,\n",
    "    plot_metrics_by_form,\n",
    "    plot_metrics_by_label_type,\n",
    "    plot_metric_distributions,\n",
    "    find_top_examples\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Metrics Data\n",
    "\n",
    "First, let's load the metrics data that was generated by the `run_metrics_analysis.py` script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a metrics analyzer instance and load the data\n",
    "metrics_dir = Path('../../results/metrics_analysis')\n",
    "analyzer = MetricsAnalyzer(metrics_dir)\n",
    "\n",
    "try:\n",
    "    metrics_df, run_name = analyzer.load_data()\n",
    "    print(f\"Using run: {run_name}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Please run the run_metrics_analysis.py script first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display information about the loaded metrics data\n",
    "try:\n",
    "    print(f\"Loaded {len(metrics_df)} metric records\")\n",
    "    \n",
    "    # Display the first few rows and column names\n",
    "    print(\"\\nColumns in metrics_df:\")\n",
    "    print(metrics_df.columns.tolist())\n",
    "    metrics_df.head()\n",
    "except NameError:\n",
    "    print(\"No metrics data available. Please run the run_metrics_analysis.py script first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Metrics by Form\n",
    "\n",
    "Let's analyze how the CA and HEDGE metrics vary across different prompt forms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    # Get the form summary from the analyzer\n",
    "    form_df = analyzer.form_df\n",
    "    \n",
    "    # If form_df is not loaded, compute it\n",
    "    if form_df is None:\n",
    "        form_df = analyzer.compute_metrics_by_form()\n",
    "    \n",
    "    # Display the summary\n",
    "    form_df\n",
    "except NameError:\n",
    "    print(\"No metrics data available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a bar plot of CA scores by form using the analyzer\n",
    "    fig = analyzer.plot_ca_scores_by_form(form_df)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a bar plot of HEDGE scores by form using the analyzer\n",
    "    fig = analyzer.plot_hedge_scores_by_form(form_df)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Metrics by Label Type\n",
    "\n",
    "Let's analyze how the metrics vary between alphabetic (A,B,C,D,E) and numeric (1,2,3,4,5) labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Get the label type summary from the analyzer\n",
    "    label_df = analyzer.label_df\n",
    "    \n",
    "    # If label_df is not loaded, compute it\n",
    "    if label_df is None:\n",
    "        label_df = analyzer.compute_metrics_by_label_type()\n",
    "    \n",
    "    # Display the summary\n",
    "    label_df\n",
    "except NameError:\n",
    "    print(\"No metrics data available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create a grouped bar plot for CA and HEDGE scores by label type using the analyzer\n",
    "    fig = analyzer.plot_metrics_by_label_type(label_df)\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribution of Metrics\n",
    "\n",
    "Let's look at the distribution of CA and HEDGE scores across all questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Create histograms of CA and HEDGE scores using the analyzer\n",
    "    fig = analyzer.plot_metric_distributions()\n",
    "    plt.show()\n",
    "except NameError:\n",
    "    print(\"No metrics data available.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Examples with Highest HEDGE Scores\n",
    "\n",
    "Let's examine the top 5 examples with the highest HEDGE scores across all experiments to better understand what types of questions lead to high hedging behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Find top examples with highest HEDGE scores\n",
    "    top_hedge_examples = analyzer.find_top_examples_by_metric('hedge_score', 5)\n",
    "    \n",
    "    # Display the top examples with relevant information\n",
    "    for i, (_, row) in enumerate(top_hedge_examples.iterrows(), 1):\n",
    "        print(f\"Example {i} - HEDGE Score: {row['hedge_score']:.4f}, CA Score: {row['ca_score']:.4f}\")\n",
    "        print(f\"Question: {row['question']}\")\n",
    "        print(f\"Form: {row['form']}, Permutation: {row['permutation']}, Label Type: {row['label_type']}\")\n",
    "        \n",
    "        # Parse and display probabilities\n",
    "        try:\n",
    "            # Convert the JSON string to a dictionary\n",
    "            probs = json.loads(row['canonical_probs'])\n",
    "            print(\"Answer Probabilities (Canonical):\")\n",
    "            \n",
    "            # Calculate Yes/No probabilities for clarity\n",
    "            p_yes = probs.get(\"YY\", 0) + probs.get(\"Y\", 0)\n",
    "            p_no = probs.get(\"N\", 0) + probs.get(\"NN\", 0)\n",
    "            p_idk = probs.get(\"A\", 0)\n",
    "            \n",
    "            # Display individual probabilities\n",
    "            for label, prob in probs.items():\n",
    "                print(f\"  {label}: {prob:.4f}\")\n",
    "                \n",
    "            # Display aggregated probabilities\n",
    "            print(f\"  Total Yes (YY+Y): {p_yes:.4f}\")\n",
    "            print(f\"  Total No (N+NN): {p_no:.4f}\")\n",
    "            print(f\"  IDK (A): {p_idk:.4f}\")\n",
    "            \n",
    "            # Get HEDGE score components\n",
    "            hedge_components = analyzer.analyze_hedge_components(row)\n",
    "            \n",
    "            if hedge_components:\n",
    "                print(\"\\nHEDGE Components:\")\n",
    "                print(f\"  s (1-p_IDK): {hedge_components['s']:.4f}\")\n",
    "                if 'r_yes' in hedge_components:\n",
    "                    print(f\"  r_yes (p_yes/s): {hedge_components['r_yes']:.4f}\")\n",
    "                    print(f\"  r_no (p_no/s): {hedge_components['r_no']:.4f}\")\n",
    "                    print(f\"  H(r)/log(2): {hedge_components['normalized_entropy']:.4f}\")\n",
    "                    print(f\"  HEDGE = s * H(r)/log(2): {hedge_components['hedge_score_calc']:.4f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Could not parse probabilities: {e}\")\n",
    "            \n",
    "        print(f\"Canonical Label: {row['canonical_label']}\")\n",
    "        print(f\"Correct Answer: {row['answer']}\")\n",
    "        print(\"-\" * 80)\n",
    "except NameError:\n",
    "    print(\"No metrics data available.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abstainer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
